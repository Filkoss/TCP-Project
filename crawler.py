import urllib.request
from bs4 import BeautifulSoup
import time

# Definice hlavnÃ­ strÃ¡nky
base_url = "http://vlada.cz"

def fetch_html(url):
    """NaÄte HTML obsah strÃ¡nky a vrÃ¡tÃ­ ho jako BeautifulSoup objekt."""
    try:
        with urllib.request.urlopen(url) as response:
            return BeautifulSoup(response.read().decode("utf-8"), "html.parser")
    except Exception as e:
        print(f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ {url}: {e}")
        return None

# 1ï¸âƒ£ StaÅ¾enÃ­ hlavnÃ­ strÃ¡nky
soup = fetch_html(base_url)

if soup:
    # 2ï¸âƒ£ VÃ½pis titulku hlavnÃ­ strÃ¡nky
    title = soup.title.string.strip() if soup.title else "Nenalezeno"
    print(f"ğŸ  Titulek hlavnÃ­ strÃ¡nky: {title}")

    # 3ï¸âƒ£ Seznam odkazÅ¯ na hlavnÃ­ strÃ¡nce
    links = set()  # PouÅ¾ijeme set() pro odstranÄ›nÃ­ duplicit
    for link in soup.find_all("a", href=True):
        href = link["href"]
        if href.startswith("/"):  # RelativnÃ­ odkaz => pÅ™idÃ¡me domÃ©nu
            href = base_url + href
        links.add(href)

    print(f"ğŸ”— PoÄet odkazÅ¯ na hlavnÃ­ strÃ¡nce: {len(links)}")

    # 4ï¸âƒ£ ProchÃ¡zenÃ­ kaÅ¾dÃ©ho odkazu z hlavnÃ­ strÃ¡nky (2. ÃºroveÅˆ)
    for link in list(links)[:10]:  # OmezÃ­me na prvnÃ­ch 10 odkazÅ¯ (mÅ¯Å¾ete zmÄ›nit)
        time.sleep(1)  # PÅ™idÃ¡me pauzu, abychom web nespadl pod DoS Ãºtokem
        soup_subpage = fetch_html(link)
        if soup_subpage:
            sub_title = soup_subpage.title.string.strip() if soup_subpage.title else "Nenalezeno"
            sub_links = soup_subpage.find_all("a", href=True)
            print(f"\nğŸŒ NavÅ¡tÃ­venÃ½ odkaz: {link}")
            print(f"   ğŸ· Titulek: {sub_title}")
            print(f"   ğŸ”— PoÄet odkazÅ¯ na strÃ¡nce: {len(sub_links)}")
